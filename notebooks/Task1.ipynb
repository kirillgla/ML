{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from sklearn.model_selection import KFold\n",
    "numpy.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_data = pandas.read_csv(\"Features_Variant_1.csv\", header=None)\n",
    "train_data.head()\n",
    "X = train_data.iloc[:, :-1]\n",
    "Y = train_data.iloc[:, -1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Calculate a few useful properties\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "\n",
    "# Avoid zero division\n",
    "X_std.loc[X_std == 0] = 1\n",
    "\n",
    "# Make feature values less deviating to avoid model instability\n",
    "X = (X - X_mean) / X_std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Fantom features that replace the non-feature-bound constant\n",
    "X.insert(loc=0, value=1, column=-1)\n",
    "# Tip: if you use PyCharm, you can Right Click + View as Data Frame\n",
    "# in the Jupyter window to interact with this data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# I mean, I know this changes semantics, but screw that, I want a clean code\n",
    "X = X.values\n",
    "Y = Y.values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Change these if you need to\n",
    "number_of_splits = 5\n",
    "number_of_epochs = 20000\n",
    "etha = 0.05"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def mean_square_error(Y, previous_Y):\n",
    "    return numpy.sum(numpy.square(Y - previous_Y)) / len(Y)\n",
    "\n",
    "def sqrt_mean_square_error(Y, previous_Y):\n",
    "    return numpy.sqrt(mean_square_error(Y, previous_Y))\n",
    "\n",
    "# Also known as R^2\n",
    "def coefficient_of_determination(Y, previous_Y):\n",
    "    return 1 - (numpy.sum(numpy.square(Y - previous_Y))) / (numpy.sum(numpy.square(Y - Y.mean())))\n",
    "\n",
    "def gradient_descent(X, Y, w):\n",
    "    delta_w = numpy.inf\n",
    "\n",
    "    iteration_index = 0\n",
    "    mean_square_error_values = list()\n",
    "\n",
    "    while delta_w > 1e-4 and iteration_index < number_of_epochs:\n",
    "        w_next = w + (2 * etha / len(Y)) * numpy.dot(X.T, Y - (numpy.dot(X, w)))\n",
    "        previous_Y = numpy.dot(X, w_next)\n",
    "\n",
    "        iteration_index += 1\n",
    "\n",
    "        mean_square_error_values.append(mean_square_error(Y, previous_Y))\n",
    "        if iteration_index % 500 == 0:\n",
    "          print(f\"Epoch {iteration_index} done.\")\n",
    "\n",
    "        delta_w = numpy.linalg.norm(w - w_next)\n",
    "        w = w_next\n",
    "\n",
    "    return w, mean_square_error_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running split option 0.\n",
      "Epoch 500 done.\n",
      "Epoch 1000 done.\n",
      "Epoch 1500 done.\n",
      "Epoch 2000 done.\n",
      "Epoch 2500 done.\n",
      "Epoch 3000 done.\n",
      "Epoch 3500 done.\n",
      "Epoch 4000 done.\n"
     ]
    }
   ],
   "source": [
    "errors = [[] for _ in range(number_of_splits)]\n",
    "weights = [[] for _ in range(number_of_splits)]\n",
    "rmse_train = [0 for _ in range(number_of_splits)]\n",
    "rmse_test = [0 for _ in range(number_of_splits)]\n",
    "r2s_train = [0 for _ in range(number_of_splits)]\n",
    "r2s_test = [0 for _ in range(number_of_splits)]\n",
    "\n",
    "kf = KFold(n_splits=number_of_splits, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for split_option_index, (train_indices, test_indices) in enumerate(kf.split(X)):\n",
    "  print(f\"Running split option {split_option_index}.\")\n",
    "  X_train, X_test = X[train_indices], X[test_indices]\n",
    "  Y_train, Y_test = Y[train_indices], Y[test_indices]\n",
    "\n",
    "  # I could randomize here, of course, but I prefer to have a reproducible results\n",
    "  weights[split_option_index], errors[split_option_index] = gradient_descent(X_train, Y_train, numpy.zeros(X.shape[1]))\n",
    "\n",
    "  train_preds = numpy.dot(X_train, weights[split_option_index])\n",
    "  test_preds = numpy.dot(X, X_test, weights[split_option_index])\n",
    "\n",
    "  rmse_train[split_option_index] = sqrt_mean_square_error(Y_train, train_preds)\n",
    "  rmse_test[split_option_index] = sqrt_mean_square_error(Y_test, test_preds)\n",
    "\n",
    "  r2s_train[split_option_index] = coefficient_of_determination(Y_train, train_preds)\n",
    "  r2s_test[split_option_index] = coefficient_of_determination(Y_test, test_preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "for error_by_iter in errors:\n",
    "  plot(range(len(error_by_iter)), error_by_iter)\n",
    "  xlabel('Iteration')\n",
    "  ylabel('mean_square_error')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columns = [\"Names\"]\n",
    "columns += [f\"T{i + 1}\" for i in range(number_of_splits)]\n",
    "columns.append(\"Mean\")\n",
    "columns.append(\"Std\")\n",
    "\n",
    "rows =  [\"sqrt_mean_square_error train\", \"sqrt_mean_square_error test\", \"coefficient_of_determination train\", \"coefficient_of_determination test\"]\n",
    "rows += [f\"Feature {i + 1}\" for i in range(len(weights[0]))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_dataframe = pandas.DataFrame(columns=columns)\n",
    "result_dataframe[\"Names\"] = rows\n",
    "result_dataframe.set_index(\"Names\", inplace=True)\n",
    "\n",
    "for i in range(number_of_splits):\n",
    "  data = numpy.concatenate(([rmse_train[i], rmse_test[i], r2s_train[i], r2s_test[i]], weights[i]))\n",
    "  result_dataframe[f\"T{i + 1}\"] = data\n",
    "\n",
    "result_dataframe[\"Mean\"] = result_dataframe.mean(axis=1)\n",
    "result_dataframe[\"Std\"] = result_dataframe.std(axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_dataframe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}